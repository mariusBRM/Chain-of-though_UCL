{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizer import *\n",
    "from generation_processing import *\n",
    "\n",
    "class MySantaCoder(nn.Module):\n",
    "    def __init__(self, list_of_bad_words = ['#'], max_tokens = 128, num_sol = 1):\n",
    "        super(MySantaCoder, self).__init__()\n",
    "        self.checkpoint = \"bigcode/santacoder\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.checkpoint, trust_remote_code=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint)\n",
    "        self.max_new_tokens = max_tokens\n",
    "        # define the list of bad word that the model should not generate\n",
    "        self.bad_words = self.get_input_ids_as_list(list_of_bad_words)\n",
    "\n",
    "        self.generation_config = GenerationConfig(\n",
    "                bad_words_ids = self.bad_words,\n",
    "                num_beams = num_sol,\n",
    "                num_return_sequences = num_sol,\n",
    "                max_new_tokens = self.max_new_tokens,\n",
    "                eos_token_id=self.model.generation_config.eos_token_id,\n",
    "                bos_token_id=self.model.generation_config.bos_token_id\n",
    "                )\n",
    "\n",
    "    def get_input_ids_as_list(self, list_of_bad_words):\n",
    "        token_list = []\n",
    "        for element in list_of_bad_words:\n",
    "            token_list.append(self.tokenizer.encode(element))\n",
    "        return token_list\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # input_ids = input_ids.unsqueeze(0)\n",
    "        outputs = self.model.generate(input_ids, self.generation_config)\n",
    "        return outputs\n",
    "\n",
    "    def decode_output(self, encoded_output):\n",
    "        output = self.tokenizer.decode(encoded_output)\n",
    "        return output\n",
    "\n",
    "    def post_generation_processing(self,code):\n",
    "        # split it into list of blocks\n",
    "        list_blocks = re.split('def |class |assert |print ', code)\n",
    "        if 'init' in list_blocks[1]:\n",
    "            fill_word = '\\nclass '\n",
    "        else:\n",
    "            fill_word = '\\ndef '\n",
    "        # keep only the first block\n",
    "        result = list_blocks[0] + fill_word + list_blocks[1]\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import * \n",
    "\n",
    "converted_mtbp = read_json_line_format('data/MTBP/converted_mtpb.jsonl')\n",
    "mtbp = read_json_line_format('data/MTBP/mtpb.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                    Append a string in the middle of another string.\n",
       "prompt                  Append a string in the middle of another string.\n",
       "code                                     def sandwich_string(A):\\r\\npass\n",
       "task_id                                                                1\n",
       "test_setup_code                                                         \n",
       "test_list              [assert sandwich_string('abcde') == ['a', 'b',...\n",
       "challenge_test_list                                                   []\n",
       "entry_point                                              sandwich_string\n",
       "test                   def check(candidate):\\n\\tassert sandwich_strin...\n",
       "signature                                        def sandwich_string(A):\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_mtbp.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prompts           [Assign the string \"{A}\" to a variable named \"...\n",
       "inputs            [{'A': 'abcde'}, {'A': 'abcdecadeCADE'}, {'A':...\n",
       "outputs           [[a, b, c, d, e], [a, b, c, d, e], [a], [ , e,...\n",
       "max_gen_length                                                128.0\n",
       "category                                                     string\n",
       "name                                                Sandwich string\n",
       "description        Append a string in the middle of another string.\n",
       "id                                                                1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtbp.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_random_name(signature):\n",
    "    # Find the function name using regular expression\n",
    "    match = re.match(r'def ([\\w\\-_%.]+)\\((.*)\\):', signature)\n",
    "    if match:\n",
    "        original_name, parameters = match.groups()\n",
    "        \n",
    "        # Generate a random name with a reasonable length (e.g., length of original name)\n",
    "        random_name = ''.join(random.choice(string.ascii_lowercase) for _ in range(len(original_name)))\n",
    "\n",
    "        # Replace the original name with the random name\n",
    "        new_signature = f'def {random_name}({parameters}):'\n",
    "        return new_signature\n",
    "    else:\n",
    "        print(signature)\n",
    "        # raise ValueError(\"Invalid function signature\")\n",
    "        return signature\n",
    "\n",
    "def custom_dataset_context_investigation(mtbp_converted, mtbp):\n",
    "\n",
    "    # select only features that are interesting\n",
    "    features_name_converted = ['text', 'signature','test_list']\n",
    "    mtbp_converted = mtbp_converted[features_name_converted]\n",
    "    features_name = ['prompts']\n",
    "    mtbp = mtbp[features_name]\n",
    "    \n",
    "    data = pd.concat([mtbp, mtbp_converted], axis=1)\n",
    "\n",
    "    random_names = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        signature = data.iloc[i]['signature']\n",
    "        random_name = generate_random_name(signature)\n",
    "        random_names.append(random_name)\n",
    "\n",
    "    data['random_signatures'] = random_names\n",
    "\n",
    "    return data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = ['def', 'if', 'for', 'while']\n",
    "\n",
    "def context_and_contexless_generation(data, model, early_stopping = None):\n",
    "    \"\"\" Generate two types of problems:\n",
    "            1. generate with the appropriate function signature and the context (keep the structural generation cut off)\n",
    "            2. generate with a random function name and without context (keep the structural generation cut off)\n",
    "            3. Keep both generation at each step with a very large cut off function (when finding a new 'def') \n",
    "    \"\"\"\n",
    "    codes_with_context = []\n",
    "    codes_without_context = []\n",
    "\n",
    "    raw_generations_context = []\n",
    "    raw_generations_no_context = []\n",
    "\n",
    "    for j in range(len(data)):\n",
    "        if early_stopping is not None and j > early_stopping:\n",
    "            break\n",
    "\n",
    "        code_with_context = []\n",
    "        code_without_context = []\n",
    "\n",
    "        no_cut_off_no_context = []\n",
    "        no_cut_off_context = []\n",
    "\n",
    "        # start with the signature for the incoming problem\n",
    "        code = data.iloc[j]['signature']\n",
    "        # start with a random name for the incoming problem\n",
    "        code_random = data.iloc[j]['random_signatures']\n",
    "        # initiate the list of prompt to generate\n",
    "        prompts = data.iloc[j]['prompts']\n",
    "        # Iterate over each prompt\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            \n",
    "            # Add the prompt to the previously generated code\n",
    "            input_text_context = code + '\\n\\t' + '#' + prompt\n",
    "            input_text_no_context = code_random +'\\n\\t' + '#' + prompt\n",
    "\n",
    "            # Encode the input text\n",
    "            input_ids_context = model.tokenizer.encode(input_text_context, return_tensors='pt')\n",
    "            input_ids_no_context = model.tokenizer.encode(input_text_no_context, return_tensors='pt')\n",
    "\n",
    "            # Generate the output\n",
    "            output_ids_context = model.forward(input_ids_context)\n",
    "            output_ids_no_context = model.forward(input_ids_no_context)\n",
    "\n",
    "            # Decode the output\n",
    "            output_text_context = model.decode_output(output_ids_context[0])\n",
    "            output_text_no_context = model.decode_output(output_ids_no_context[0])\n",
    "\n",
    "\n",
    "\n",
    "            # Cut off the generated code\n",
    "            code = generation_cut_off(gen_code = output_text_context, stop_words=STOP_WORDS, index_prompt=i)\n",
    "            code_random = generation_cut_off(gen_code = output_text_no_context, stop_words=STOP_WORDS, index_prompt=0)\n",
    "            code_random = remove_context(code_random)\n",
    "\n",
    "            # Keep the generation with a large cut off (new def found)\n",
    "            output_text_context = model.post_generation_processing(output_text_context)\n",
    "            output_text_no_context = model.post_generation_processing(output_text_no_context)\n",
    "\n",
    "            code_with_context.append(code)\n",
    "            code_without_context.append(code_random)\n",
    "\n",
    "            no_cut_off_no_context.append(output_text_no_context)\n",
    "            no_cut_off_context.append(output_text_context)\n",
    "\n",
    "        codes_with_context.append(code_with_context)\n",
    "        codes_without_context.append(code_without_context)\n",
    "\n",
    "        raw_generations_context.append(no_cut_off_context)\n",
    "        raw_generations_no_context.append(no_cut_off_no_context)\n",
    "\n",
    "    return codes_with_context, raw_generations_context, codes_without_context, raw_generations_no_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import *\n",
    "mtbp_converted = read_json_line_format('data/MTBP/converted_mtpb.jsonl')\n",
    "mtbp = read_json_line_format('data/MTBP/mtpb.jsonl')\n",
    "\n",
    "dataset = custom_dataset_context_investigation(mtbp_converted, mtbp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prompts              [Assign the string \"{A}\" to a variable named \"...\n",
       "text                  Append a string in the middle of another string.\n",
       "signature                                      def sandwich_string(A):\n",
       "test_list            [assert sandwich_string('abcde') == ['a', 'b',...\n",
       "random_signatures                              def uxbwfshjwzjglbh(A):\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mariu\\OneDrive\\Bureau\\UCL\\T3\\Chain-of-though_UCL\\src\\context_exploration.ipynb Cell 11\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mariu/OneDrive/Bureau/UCL/T3/Chain-of-though_UCL/src/context_exploration.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m MySantaCoder()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mariu/OneDrive/Bureau/UCL/T3/Chain-of-though_UCL/src/context_exploration.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m codes_with_context, codes_without_context \u001b[39m=\u001b[39m context_and_contexless_generation(data\u001b[39m=\u001b[39mdataset, model\u001b[39m=\u001b[39mmodel, early_stopping\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MySantaCoder()\n",
    "codes_with_context, codes_without_context, codes_without_context, raw_generations_no_context = context_and_contexless_generation(data=dataset, model=model, early_stopping=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def normalize_integer_list(numbers):\\n\\t#Define a list of integers named \"numbers\" with the values {numbers}.\\n\\tnumbers = [int(x) for x in numbers]',\n",
       " 'def normalize_integer_list(numbers):\\n\\t#Calculate the sum of the elements in variable \"numbers\" and store the result to variable \"total\".\\n\\ttotal = sum(numbers)',\n",
       " 'def normalize_integer_list(numbers):\\n\\t#Divide each element of the list by the total and multiply by 100, store the result to variable \"normalized\".\\n\\tnormalized = []',\n",
       " 'def normalize_integer_list(numbers):\\n\\t#Convert each element in variable \"normalized\" into a formatted string with single decimal point and store the result into \"formatted\".\\n\\tformatted = []',\n",
       " 'def normalize_integer_list(numbers):\\n\\t#Print the variable \"formatted\".\\n\\tformatted = []']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes_with_context[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def wicupvsftvndphmdqgwdyw(numbers):\\n\\tnumbers = [int(x) for x in numbers]',\n",
       " 'def wicupvsftvndphmdqgwdyw(numbers):\\n\\ttotal = 0',\n",
       " 'def wicupvsftvndphmdqgwdyw(numbers):\\n\\tnormalized = []',\n",
       " 'def wicupvsftvndphmdqgwdyw(numbers):\\n\\tformatted = \"\"',\n",
       " 'def wicupvsftvndphmdqgwdyw(numbers):\\n\\tformatted = \"\"']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes_without_context[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0197-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
